La modalità tradizionale di distribuzione nell’ambito dell’\textit{Infrastructure-as-a-Service} (\textit{IaaS}) richiede che un server sia attivo a lungo termine per garantire servizi continui. Tuttavia, questa allocazione esclusiva implica che le risorse vengano mantenute anche quando l’applicazione non è in esecuzione. Di conseguenza, l'utilizzo delle risorse nei data center è generalmente basso, attestandosi in media intorno al 10\%, specialmente per i servizi online con un uso prevalentemente diurno. Questa inefficienza ha portato allo sviluppo di un modello di servizio on-demand gestito dalla piattaforma, con l’obiettivo di migliorare l’utilizzo delle risorse e ridurre i costi del cloud computing.

Al momento non esiste una definizione ufficiale di serverless computing. Tuttavia, le definizioni comunemente accettate, come quelle proposte dal Berkeley View, lo descrivono come segue:
\begin{itemize}
    \item \textit{Serverless Computing = FaaS (Function-as-a-Service) + BaaS (Backend-as-a-Service)}. Esiste un malinteso comune secondo cui il termine \textit{serverless} può essere usato in modo intercambiabile con \textit{FaaS}. In realtà, entrambi sono elementi fondamentali per il serverless computing. Il modello FaaS permette l'isolamento e l'invocazione delle singole funzioni, mentre il modello BaaS fornisce il supporto backend necessario per i servizi online.
    \item Nel modello \textit{FaaS}, noto anche come paradigma \textit{Lambda}, un’applicazione viene scomposta in funzioni o microservizi a livello di funzione. Gli aspetti principali che caratterizzano una funzione includono l'identificatore della funzione, il runtime del linguaggio, il limite di memoria per ciascuna istanza e l'URI (Uniform Resource Identifier) che definisce il codice della funzione.
    \item \textit{BaaS} rappresenta un insieme di servizi essenziali su cui si basano le applicazioni. Alcuni esempi includono lo storage, i servizi di notifica dei messaggi e gli strumenti per il DevOps.
\end{itemize}
In sintesi, serverless computing combina sia il modello FaaS che quello BaaS, fornendo una struttura versatile per lo sviluppo e l'esecuzione di applicazioni senza la necessità di gestire direttamente l'infrastruttura sottostante.

Le funzioni cloud rappresentano quindi il fondamento del serverless computing e stanno promuovendo un modello di programmazione più semplice e versatile per il cloud. Grazie alla loro capacità di essere eseguite in risposta a eventi e richieste, le funzioni cloud permettono agli sviluppatori di concentrarsi sulla logica applicativa senza preoccuparsi della gestione dell'infrastruttura sottostante. Questo approccio semplificato consente di sviluppare, distribuire e scalare applicazioni in modo più efficiente, aprendo la strada a un paradigma di programmazione cloud più flessibile e accessibile.

Per comprendere al meglio il modello di elaborazione serverless, si consideri un esempio basato su un'invocazione asincrona di una funzione serverless. In questo scenario, il sistema serverless riceve le API innescate e le invocazioni vengono gestite attraverso un sistema di notifica. Il sistema serverless processa le richieste API inviate dagli utenti, le valida e avvia le funzioni, creando nuove sandbox per le invocazioni (chiamato \textit{cold startup}) oppure riutilizzando sandbox già attive (chiamato \textit{warm startup}). Ogni invocazione di funzione viene eseguita in isolamento, all'interno di un container individuale o di una macchina virtuale, che viene assegnata da un controllore di accesso per garantire la sicurezza e l'isolamento tra le invocazioni.

Grazie alla natura \textit{event-driven} e all’elaborazione basata su singoli eventi, il sistema serverless può essere attivato su richiesta, creando istanze isolate in risposta agli eventi e scalando orizzontalmente in base al carico effettivo dell'applicazione. Successivamente, ogni worker che esegue le funzioni accede a un database di backend per memorizzare i risultati dell'elaborazione. Gli utenti possono inoltre personalizzare l'esecuzione di applicazioni complesse configurando trigger aggiuntivi e definendo interazioni tra eventi, costruendo pipeline di eventi interni per gestire flussi di lavoro articolati.
